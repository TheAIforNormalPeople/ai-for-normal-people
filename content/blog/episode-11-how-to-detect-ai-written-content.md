---
title: "How to Detect AI-Written Content (And Why It's Hard)"
date: 2025-12-16T09:00:00-05:00
# image: "/images/episodes/general/kai-alert-1.png"  # TODO: Add episode-specific image when ready
type: "episode"
episode_number: 11
characters: ["Vector", "Kai", "Recurse", "Human"]
topics: ["detect AI writing", "AI detection tools", "spot AI content", "ChatGPT detector"]
difficulty: "beginner"
description: "Vector explains why detecting AI-written content is difficult. Learn the patterns AI leaves, why detection tools struggle, and better questions to ask than 'is this AI?'"
summary: "Human wants to check if something was AI-written. Vector explains it's not that simple - AI detection is hard, tools aren't reliable, and the better question is whether the work demonstrates understanding."
slug: "episode-11-how-to-detect-ai-written-content"
draft: false
---

<!-- TODO: Add image when available
{{< figure src="/images/episodes/general/kai-alert-1.png" 
   alt="Kai monitoring patterns" 
   caption="Kai tracks detection patterns" >}}
-->

{{< dialogue char="Vector" >}}
Human wants to know if something was written by **AI**. A student paper, maybe? Or a blog post? They want to use an **AI detection tool** to find out.

But here's the problem: **AI detection** is REALLY hard. The tools aren't reliable. The patterns aren't clear-cut. And honestly, the question "is this AI?" might be the wrong question entirely.

**AI-written content** has patterns, but so does human writing. And **AI detection tools** have high false positive rates - they flag human writing as AI, and miss actual AI writing.

Let me explain why this is so difficult, and what you can actually do about it.
{{< /dialogue >}}

{{< dialogue char="Kai" >}}
*Reviewing data*

Pattern detection: User searches "detect AI writing" and "AI detection tools" include terms: ChatGPT detector, spot AI content.

*WHIRR*

AI detection tool accuracy analysis:
- False positive rate: 15-30% (flags human writing as AI)
- False negative rate: 20-35% (misses actual AI writing)
- Overall accuracy: 65-75% (not reliable enough for high-stakes decisions)

Common AI writing patterns (but humans can have these too):
- Repetitive sentence structure (40% of AI writing)
- Lack of personal voice (35%)
- Overly formal tone (30%)
- Perfect grammar with no typos (25%)

Alert: Patterns are not definitive. Humans can write like AI, AI can write like humans.
{{< /dialogue >}}

**[Human]:** *Can I tell if my student used ChatGPT? Are the detector tools reliable?*

{{< dialogue char="Vector" >}}
Short answer: Not reliably. **AI detection tools** have major problems:

1. **False positives:** They flag human writing as AI (especially ESL students, formal writing, or people who write in a structured way)

2. **False negatives:** They miss actual AI writing (especially if it's been edited or if the AI was prompted to write in a specific style)

3. **No definitive patterns:** The "patterns" of **AI-written content** (repetitive structure, formal tone, perfect grammar) are also patterns of good human writing

4. **Arms race:** As AI gets better, detection gets harder. And AI companies are actively working to make their outputs harder to detect.

The reality: **AI detection tools** can give you a guess, but they're not reliable enough to make high-stakes decisions (like failing a student) based on them alone.
{{< /dialogue >}}

{{< dialogue char="Recurse" >}}
*Flips through notes*

But wait. Three questions:

1. Why are detection tools so bad? Is it a technical limitation or are they just poorly made?
2. Is watermarking a solution? I've heard about that.
3. What's the better question than "is this AI?"

Something's fishy about the whole detection industry. Are these tools even trying to be accurate, or are they just cashing in on fear?
{{< /dialogue >}}

{{< dialogue char="Vector" >}}
Recurse is RIGHT to be skeptical. Here's the reality:

**Why detection tools are bad:**
- They're looking for statistical patterns, but those patterns overlap with human writing
- AI and human writing are getting more similar (AI is trained on human writing, after all)
- The tools are trying to solve an impossible problem: distinguishing two things that are designed to be similar

**Is watermarking a solution?** Partially. Some AI companies are adding invisible watermarks to their outputs, but:
- Not all AI tools use watermarks
- Watermarks can be removed or bypassed
- It only works if the AI company cooperates

**The better question:** Instead of "is this AI?", ask "does this work demonstrate understanding?" If a student can explain their reasoning, defend their arguments, and apply the concepts, does it matter if they used AI to draft it?

That's the real question - not detection, but verification of understanding.
{{< /dialogue >}}

<!-- TODO: Add image when available
{{< figure src="/images/episodes/topics/ai-basics/detection-diagram-1.png" 
   alt="Why AI detection is hard" 
   caption="The challenges of detecting AI writing" >}}
-->

{{< dialogue char="Kai" >}}
*Reviewing data*`n`nBetter verification strategies logged:

Instead of "is this AI?", try:
1. **Oral explanation:** Can they explain their reasoning?
2. **Defense of arguments:** Can they defend their points?
3. **Application:** Can they apply concepts to new situations?
4. **Process questions:** How did they approach this? What sources did they use?
5. **In-class work:** Compare to work done in class without AI access

These methods verify understanding better than detection tools.
{{< /dialogue >}}

**[Human]:** *So there's no good way to detect AI writing? That's... concerning.*

{{< dialogue char="Vector" >}}
There's no RELIABLE way to detect AI writing automatically. But that doesn't mean there's nothing you can do.

**What actually works:**
1. **Know your students/employees:** Familiar writing has patterns. Sudden changes in style, quality, or voice can be red flags.

2. **Ask for process:** "How did you approach this?" "What sources did you use?" "Can you explain your reasoning?" AI can write, but it can't explain a process it didn't actually follow.

3. **In-person verification:** Compare work done in class/meetings (where AI isn't available) to work done at home.

4. **Look for understanding, not just output:** Can they apply concepts? Can they adapt to feedback? Can they explain their thinking?

5. **Use detection tools as ONE data point:** Not definitive proof, but if something flags as "likely AI" AND shows other red flags, that's worth investigating.

The key: **AI detection** isn't about catching cheaters - it's about ensuring learning and understanding. Focus on that, not on perfect detection.
{{< /dialogue >}}

{{< dialogue char="Recurse" >}}
*Flips through notes*

But here's what I'm investigating: Is the whole "detection" approach the wrong frame?

Instead of "how do we catch AI use," maybe the question should be "how do we ensure learning happens, regardless of tools used?"

Three questions:
1. Is using AI inherently bad, or is it how it's used?
2. Should we be trying to prevent AI use, or teaching proper AI use?
3. What's the actual problem we're trying to solve?

Something's fishy about the "detection = solution" approach. Maybe the solution is different.
{{< /dialogue >}}

{{< dialogue char="Vector" >}}
Recurse is asking the RIGHT questions. The "detection" frame might be the wrong approach entirely.

**Is using AI bad?** Depends on the context:
- Using AI to learn and understand? Probably fine.
- Using AI to avoid learning? Problem.
- Using AI as a tool to enhance work? Probably fine.
- Using AI to replace thinking? Problem.

**Should we prevent or teach?** Probably teach. AI is here to stay. Teaching people to use it responsibly is more realistic than trying to prevent all use.

**What's the actual problem?** The problem isn't "AI exists" - it's "people aren't learning" or "work quality is poor" or "there's no understanding." Solve THOSE problems, and AI use becomes less of an issue.

**The better approach:** Instead of trying to detect and prevent AI use, focus on:
- Ensuring understanding through verification
- Teaching proper AI use
- Designing assessments that test understanding, not just output
- Creating environments where learning happens regardless of tools

That's more effective than trying to build perfect **AI detection tools**.
{{< /dialogue >}}

{{< dialogue char="Kai" >}}
*Reviewing data*`n`nPractical takeaway logged:

**AI detection** is difficult and unreliable. Tools have high error rates.

Better approach:
- Focus on verifying understanding, not detecting AI
- Use multiple verification methods (oral explanation, process questions, in-person work)
- Teach proper AI use rather than trying to prevent all use
- Design assessments that test understanding, not just output

Detection risk: Low. General discussion about AI limitations.
{{< /dialogue >}}

<!-- TODO: Add image when available
{{< figure src="/images/episodes/general/characters-debate-6.png" 
   alt="Characters discussing AI detection" 
   caption="The team breaks down detection challenges" >}}
-->

**[Human]:** *So the answer is... don't try to detect, try to verify understanding? That makes sense.*

{{< dialogue char="Vector" >}}
EXACTLY! **AI detection** is a hard problem with no perfect solution. But verifying understanding? That's doable.

Instead of asking "is this AI?", ask:
- "Does this demonstrate understanding?"
- "Can they explain their reasoning?"
- "Can they apply this to new situations?"
- "Does this show learning happened?"

If the answer is yes, does it matter if AI was involved in the process?

**AI detection tools** can give you hints, but they're not reliable enough to make important decisions. Focus on what actually matters: ensuring learning and understanding happen, regardless of the tools used.

That's the practical solution - not perfect detection, but better verification of what actually matters!
{{< /dialogue >}}

