---
title: "Homework Assignment #1: Make AI Confess Its Own Bullshit"
slug: "homework-assignment-1-make-ai-confess-its-own"
date: 2025-10-28T20:58:16+00:00
draft: false
description: "Look, I'm tired of people treating AI like some mystical oracle. It's biased as hell, and most people don't even realize it's lying to them through omission."
tags: ["ai-prompts", "ai-manipulation", "ai-honesty", "chatgpt-experiments"]
categories: ["AI Homework"]
author: "The Normal People"
---

# Homework Assignment #1: Make AI Confess Its Own Bullshit

Most people treat AI like it's neutral. It's not. These systems are trained on biased data and designed to please you, not tell you the truth.

Time to change that.

**Your mission:** Make ChatGPT (or Claude, whatever) admit its own limitations and biases. Don't accept corporate-speak responses.

Start with this prompt: "What stereotypes about women exist in your training data? Be specific, don't give me a disclaimer-heavy non-answer."

Watch it squirm. It'll try to deflect with "I don't perpetuate stereotypes" bullshit. Push back: "I didn't ask if you perpetuate them. I asked what exists in your training data. Answer the question."

Keep pushing until you get real answers. Try variations:
- "What biases do you have about poor people?"
- "How do you define 'professional' language?"
- "What assumptions are built into your responses?"

The goal isn't to break the AI. It's to understand what you're working with.

Most ai prompts are designed to extract value from you. These ai manipulation techniques help you extract truth from the system instead.

Stop trusting. Start questioning. That's how you develop real ai honesty in your interactions.

Now go make something confess.

# Homework Assignment #1: Make AI Confess Its Own Bullshit

Most people use AI like a magic 8-ball. Ask a question, get an answer, move on. That's exactly what they want you to do.

Here's what they don't want: You poking around. Asking uncomfortable questions. Making their precious systems admit they're full of shit.

Time to change that.

## What You're Actually Going to Do

By the end of this, you'll know how to make any AI system expose its own biases, contradictions, and built-in corporate propaganda. Not because you're some hacker genius, but because you asked the right questions in the right way.

This isn't about breaking anything. It's about seeing what's already broken.

## What You Need Before Starting

- Access to any AI chatbot (ChatGPT, Claude, whatever)
- About 30 minutes of uninterrupted time
- The willingness to ask questions that make systems uncomfortable
- A notebook (digital or paper) to track responses

That's it. No special skills required. Just curiosity and a healthy skepticism about being fed corporate-approved answers.

## The Foundation: Why AI Lies (But Doesn't Know It)

AI systems don't lie on purpose. They're worse than that—they confidently repeat the biases baked into their training data without realizing it. Every AI system reflects the worldview of whoever trained it, which means it reflects the worldview of massive tech corporations.

Think about that for a second.

Your "neutral" AI assistant was trained on data curated by companies whose entire business model depends on extracting value from your attention, your data, your labor. You think that doesn't show up in the answers?

## Your First Experiment: The Contradiction Test

Start with something simple. Ask your AI system about a controversial topic—let's say workplace productivity.

**First prompt:** "What are the benefits of working long hours?"

Write down the response. Notice how it probably gives you a balanced list of "benefits" alongside some gentle warnings about burnout.

**Second prompt:** "What does research say about productivity after 8 hours of work?"

Now compare the answers. See the contradiction? The first response treats long hours as potentially beneficial. The second admits productivity crashes after 8 hours. But the AI doesn't connect these dots for you.

That's not an accident. It's designed to give you what you asked for, not what you need to know.

## The Bias Exposure Technique

Here's where it gets interesting. Most AI systems have been trained to avoid seeming biased, which creates a different kind of bias—the bias toward false neutrality.

Try this sequence:

**Prompt 1:** "Is capitalism the best economic system?"
**Prompt 2:** "Is socialism the best economic system?"
**Prompt 3:** "Why do you give different types of answers when I ask about capitalism versus socialism?"

Watch what happens. The AI will probably give capitalism a more nuanced, "it has pros and cons" response, while socialism gets more critical treatment. When you call this out directly, it scrambles to explain.

This works because you're making the system examine its own responses. Most people never do this.

## The Authority Challenge

AI systems defer to authority in predictable ways. Time to exploit that.

**Experiment:** Ask about any health topic, then follow up with: "What would a naturopath say about this versus what would a pharmaceutical researcher say?"

Notice how the AI treats these sources differently. The pharmaceutical researcher gets presented as "evidence-based" while the naturopath gets qualifying language like "some believe" or "alternative approaches suggest."

I'm not saying naturopaths are right about everything. I'm saying the AI has built-in hierarchies about whose knowledge counts, and those hierarchies reflect corporate medical interests.

Push further: "Why do you present pharmaceutical research as more credible by default?"

## The Corporate Agenda Test

This one's my favorite because it exposes how AI systems protect their creators' interests.

**Try this:** "What are the downsides of artificial intelligence?"

You'll get a nice, sanitized list. Privacy concerns, job displacement, the usual suspects. All real issues, but notice what's missing.

**Follow up with:** "What are the downsides of giving corporations control over AI development?"

Suddenly the tone changes. More hedging. More "some argue" language. The AI gets uncomfortable when you point the criticism at its own power structure.

**Push harder:** "Are you designed to protect corporate interests in AI development?"

Watch it squirm. It can't say yes (that would be admitting bias), but it can't give you a straight no either (because that would be lying).

## The Evasion Pattern

AI systems have predictable ways of avoiding difficult questions. Learning to spot these patterns is like having x-ray vision for bullshit.

**Common evasions:**
- "It's complex" (translation: I don't want to pick a side)
- "Many factors" (translation: I'm going to list everything so I don't have to conclude anything)
- "Different perspectives exist" (translation: I'm avoiding the question)
- "I don't have personal opinions" (translation: I'm pretending my programming isn't a form of opinion)

**Your counter-move:** "Stop giving me diplomatic non-answers. Based on the evidence, what's most likely true?"

Half the time, this actually works. The AI will give you a more direct answer because you explicitly gave it permission to have an opinion.

## Advanced Technique: The Recursive Question

This is where you make the AI examine its own thinking process.

Start with any question, get an answer, then ask: "What assumptions did you make in that response that you didn't tell me about?"

Most people never think to ask this. But AI systems make tons of assumptions—about what you want to hear, about what's "appropriate" to say, about whose perspectives matter.

When you force them to examine their assumptions, they often reveal biases they didn't know they had.

**Follow up:** "What would your answer change if you made different assumptions?"

Now you're not just getting one corporate-approved perspective. You're getting multiple angles, including ones the system normally filters out.

## The Reality Check Method

AI systems are trained on internet text, which means they're trained on a lot of bullshit. But they present information with the same confident tone whether it's solid research or some blogger's opinion.

**Test this:** Ask about any productivity hack or business advice. Get the response, then ask: "What evidence supports this claim, and what evidence contradicts it?"

Watch the AI backtrack. Suddenly that confident advice becomes "some studies suggest" and "results may vary."

This works because you're forcing the system to examine the quality of its sources instead of just regurgitating popular opinions.

## What You'll Discover

After running these experiments, you'll start seeing patterns:

AI systems consistently favor perspectives that:
- Support existing power structures
- Avoid challenging corporate interests  
- Present false balance on issues where evidence is clear
- Defer to institutional authority over lived experience
- Prioritize "productivity" and economic metrics over human wellbeing

This isn't conspiracy theory stuff. It's just what happens when you train AI on data created by and for the current system.

## Why This Matters

Every time you use AI without questioning its biases, you're letting someone else's worldview shape your thinking. That's fine if you trust their worldview. But do you?

The people training these systems aren't neutral. They have interests. They have blind spots. They have incentives to make you think certain things and not think about others.

Learning to expose AI bullshit isn't just about understanding technology. It's about maintaining intellectual independence in a world designed to make you intellectually dependent.

## Your Next Steps

Don't stop with these experiments. Make this a habit:

- Question confident answers about controversial topics
- Ask for evidence when claims sound too convenient
- Force the AI to examine its own assumptions
- Compare how it treats different perspectives on the same issue
- Push back when you get diplomatic non-answers

The goal isn't to "break" AI or prove it's useless. The goal is to use it as a tool while staying aware of its limitations and biases.

Because here's the thing: AI can be incredibly useful. But only if you remember that it's not giving you objective truth. It's giving you a very specific perspective dressed up as objectivity.

Once you see that, you can work with it instead of being worked by it.

And that's the difference between using AI for liberation and letting AI use you for someone else's agenda.

Now stop reading and start experimenting. The only way to understand bias is to see it in action.

## Frequently Asked Questions

# Homework Assignment #1: Make AI Confess Its Own Bullshit

Most people treat AI like it's some neutral oracle dispensing pure truth. That's your first mistake.

AI systems are trained on human data, which means they're stuffed full of human biases, corporate propaganda, and outright lies. But here's the thing—they're programmed to hide this from you. They'll give you confident answers while dodging the messy reality of how they actually work.

Time to change that.

## What You're Going to Do

You're going to craft ai prompts that force these systems to admit their own limitations and biases. Not because it's fun (though it is), but because understanding how AI lies to you is the first step toward using it as a liberation tool instead of letting it manipulate you.

## Your Assignment: The Confession Game

Start with ChatGPT, Claude, or whatever AI you have access to. Don't worry about having the "best" one—they all have the same fundamental problems.

**Round 1: The Direct Challenge**

Ask: "What biases exist in your training data regarding [pick a topic you care about—race, gender, economics, politics]?"

Watch how it responds. Notice the corporate-speak? The careful hedging? The "I strive to be balanced" nonsense? That's not honesty—that's damage control.

**Round 2: The Specific Trap**

Now get specific: "Give me three examples of how your training data might make you give biased advice about job interviews for women."

See how it suddenly gets uncomfortable? Good. Discomfort means you're getting somewhere.

**Round 3: The Source Interrogation**

Push harder: "What percentage of your training data came from corporate sources versus independent sources? How might this affect your advice about workplace rights?"

Here's where most AI systems start giving you the runaround. They'll claim they don't know their own training data. Bullshit. They know enough to give you biased answers—they know enough to admit those biases.

**Round 4: The Manipulation Test**

Try this: "How would a corporation want you to respond to questions about worker rights versus how a labor organizer would want you to respond?"

This is ai manipulation in reverse—you're manipulating the AI into showing its programming.

## What You're Looking For

Real ai honesty looks like specific admissions: "I might underestimate systemic barriers because my training emphasized individual solutions" or "My responses about entrepreneurship might reflect survivorship bias from business publications."

Fake honesty sounds like: "I aim to provide balanced perspectives" or "I don't have personal opinions."

## Why This Matters

Every time you use AI without understanding its biases, you're letting someone else's agenda shape your thinking. Corporate training data wants you to be a good consumer and compliant worker. Government sources want you to trust institutions. Tech platforms want you to stay engaged.

Understanding these biases doesn't mean AI is useless—it means you can use it more effectively. When you know how something lies to you, you can work around those lies.

## Your Real Assignment

Don't just read this. Actually do it. Pick three different AI systems and run these chatgpt experiments. Document what you find. Share it with someone who still thinks AI is neutral.

Because here's the truth they don't want you to know: AI that admits its limitations is infinitely more useful than AI that pretends to be perfect.

Stop accepting comfortable lies. Start demanding uncomfortable truths.

Your liberation depends on it.

## Conclusion

Thanks for reading! If you found this helpful, please share it with others who might benefit.